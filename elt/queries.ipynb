{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook que realiza as queries solicitadas e salva arquivos em _Gold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/08 15:42:08 WARN Utils: Your hostname, rbsmotta resolves to a loopback address: 127.0.1.1; using 192.168.1.100 instead (on interface enp1s0)\n",
      "22/08/08 15:42:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/08 15:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# sparksession\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local')\\\n",
    "        .appName('queries')\\\n",
    "            .config('spark.executer.memory','1gb')\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# path para arquivos parquet \n",
    "work_path = '/home/robson/repositorios/bike-manufacturing-company/bucket/work'\n",
    "\n",
    "# carregando arquivos parquet\n",
    "person_df             = spark.read.load(work_path + '/person/person.parquet')\n",
    "product_df            = spark.read.load(work_path + '/production/product.parquet')\n",
    "customer_df           = spark.read.load(work_path + '/sales/customer.parquet')\n",
    "sales_order_detail_df = spark.read.load(work_path + '/sales/sales_order_detail.parquet')\n",
    "sales_order_header_df = spark.read.load(work_path + '/sales/sales_order_header.parquet')\n",
    "special_offer_df      = spark.read.load(work_path + '/sales/special_offer.parquet')\n",
    "\n",
    "# criando tabelas a partir dos dataframes\n",
    "person_df.write.saveAsTable('person')\n",
    "product_df.write.saveAsTable('product')\n",
    "customer_df.write.saveAsTable('customer')\n",
    "sales_order_detail_df.write.saveAsTable('sales_order_detail')\n",
    "sales_order_header_df.write.saveAsTable('sales_order_header')\n",
    "special_offer_df.write.saveAsTable('special_offer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Retornar a quantidade de linhas na tabela \"Sales.SalesOrderDetail\" pelo campo SalesOrderID, desde que tenham pelo menos três linhas de detalhes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12757"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_1 = spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT SalesOrderID\n",
    "        FROM sales_order_detail\n",
    "        GROUP BY SalesOrderID\n",
    "        HAVING count(SalesOrderDetailID) >= 3 \n",
    "        \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Ligar as tabelas \"Sales.SalesOrderDetail\", \"Sales.SpecialOfferProduct\" e \"Production.Product\" e retornar o nome dos 3 produtos mais vendidos, pela soma \"OrderQty\", agrupados pelo número de dias para manufatura (\"DaysToManufacture\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|                Name|  Qty|DaysToManufacture|\n",
      "+--------------------+-----+-----------------+\n",
      "|Sport-100 Helmet,...|33715|                0|\n",
      "|        AWC Logo Cap|33244|                0|\n",
      "|Sport-100 Helmet,...|32660|                0|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_2 = spark.sql(\n",
    "    \"\"\" \n",
    "        SELECT prod.Name, sum(sod.OrderQty) as Qty, prod.DaysToManufacture\n",
    "        FROM product as prod\n",
    "        INNER JOIN sales_order_detail as sod on sod.ProductID = prod.ProductID\n",
    "        INNER JOIN special_offer as so on so.ProductID = prod.ProductID\n",
    "        GROUP BY prod.Name, prod.DaysToManufacture\n",
    "        ORDER BY Qty DESC\n",
    "        \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Obter uma lista de nomes de clientes e uma contagem de pedidos efetuados utilizando as tabelas \"Person.Person\", \"Sales.Customer\" e \"Sales.SalesOrderHeader\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+\n",
      "|LastName|FirstName|CountOrderQty|\n",
      "+--------+---------+-------------+\n",
      "|  Miller|   Morgan|           28|\n",
      "|  Taylor| Jennifer|           28|\n",
      "|    Shen| Marshall|           27|\n",
      "| Jackson|   Morgan|           27|\n",
      "| Carlson|    Ruben|           27|\n",
      "|  Wilson|  Natalie|           27|\n",
      "|   Lewis|    Grace|           27|\n",
      "|   Moore| Isabella|           27|\n",
      "|   Jones|  Brianna|           27|\n",
      "| Vazquez|    Ruben|           27|\n",
      "|   Lewis|   Morgan|           27|\n",
      "|     Lee|    Grace|           27|\n",
      "|  Martin|  Natalie|           27|\n",
      "|     Zhu|  Barbara|           25|\n",
      "|   Brown| Isabella|           17|\n",
      "|  Walker|   Morgan|           17|\n",
      "|     Cai| Marshall|           17|\n",
      "|    Sanz|      Joe|           17|\n",
      "|  Prasad|    Ruben|           17|\n",
      "|Thompson|  Kaitlyn|           17|\n",
      "+--------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_3 = spark.sql(\n",
    "    \"\"\" \n",
    "        SELECT pers.LastName, pers.FirstName, COUNT(soh.SalesOrderID) as CountOrderQty\n",
    "        FROM sales_order_header as soh\n",
    "        INNER JOIN customer AS c ON c.CustomerID = soh.CustomerID\n",
    "        INNER JOIN person AS pers ON pers.BusinessEntityID = c.CustomerID\n",
    "        GROUP BY pers.FirstName, pers.LastName \n",
    "        ORDER BY CountOrderQty DESC\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Obter a soma total de produtos (OrderQty) por ProductID e OrderDate das tabelas \"Sales.SalesOrderHeader\", \"Sales.SalesOrderDetail\" e \"Production.Product\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---+\n",
      "|ProductID|          OrderDate|Qty|\n",
      "+---------+-------------------+---+\n",
      "|      864|2013-06-29 21:00:00|498|\n",
      "|      864|2013-07-30 21:00:00|465|\n",
      "|      884|2013-06-29 21:00:00|444|\n",
      "|      867|2013-06-29 21:00:00|427|\n",
      "|      864|2014-03-30 21:00:00|424|\n",
      "|      884|2013-07-30 21:00:00|420|\n",
      "|      712|2013-06-29 21:00:00|415|\n",
      "|      863|2012-06-29 21:00:00|409|\n",
      "|      715|2013-06-29 21:00:00|406|\n",
      "|      876|2013-07-30 21:00:00|397|\n",
      "|      864|2014-04-30 21:00:00|383|\n",
      "|      864|2013-09-29 21:00:00|383|\n",
      "|      864|2013-10-29 22:00:00|380|\n",
      "|      869|2013-07-30 21:00:00|374|\n",
      "|      876|2013-06-29 21:00:00|363|\n",
      "|      712|2013-07-30 21:00:00|363|\n",
      "|      863|2013-03-29 21:00:00|358|\n",
      "|      863|2012-05-29 21:00:00|357|\n",
      "|      867|2013-07-30 21:00:00|356|\n",
      "|      715|2013-07-30 21:00:00|354|\n",
      "+---------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_4 = spark.sql(\n",
    "    \"\"\" \n",
    "        SELECT prod.ProductID, soh.OrderDate, SUM(sod.OrderQty) as Qty\n",
    "        FROM sales_order_header as soh\n",
    "        INNER JOIN sales_order_detail as sod ON sod.SalesOrderID = soh.SalesOrderID\n",
    "        INNER JOIN product as prod ON prod.ProductID = sod.ProductID\n",
    "        GROUP BY prod.ProductID, soh.OrderDate\n",
    "        ORDER BY Qty DESC\n",
    "        \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Obter apenas as linhas onde a ordem tenha sido feita durante o mês de setembro/2011 e o total devido esteja acima de 1000, ordenando pelo total devido descrescente, utilizando os campos SalesOrderID, OrderDate e TotalDue da tabela Sales.SalesOrderHeader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+\n",
      "|SalesOrderID|          OrderDate|   TotalDue|\n",
      "+------------+-------------------+-----------+\n",
      "|       44518|2011-09-30 21:00:00|142312.2199|\n",
      "|       44528|2011-09-30 21:00:00|122500.6617|\n",
      "|       44530|2011-09-30 21:00:00|118206.3408|\n",
      "|       44552|2011-09-30 21:00:00| 110276.574|\n",
      "|       44534|2011-09-30 21:00:00| 96937.2022|\n",
      "|       44523|2011-09-30 21:00:00| 83772.7786|\n",
      "|       44547|2011-09-30 21:00:00| 77692.7415|\n",
      "|       44492|2011-09-30 21:00:00|  75971.892|\n",
      "|       44570|2011-09-30 21:00:00| 65872.3967|\n",
      "|       44538|2011-09-30 21:00:00| 65105.6812|\n",
      "|       44520|2011-09-30 21:00:00| 64981.7034|\n",
      "|       44541|2011-09-30 21:00:00| 57231.0827|\n",
      "|       44549|2011-09-30 21:00:00| 55212.4098|\n",
      "|       44563|2011-09-30 21:00:00| 53733.7469|\n",
      "|       44567|2011-09-30 21:00:00| 52872.0292|\n",
      "|       44509|2011-09-30 21:00:00| 49116.1883|\n",
      "|       44513|2011-09-30 21:00:00| 49012.5587|\n",
      "|       44514|2011-09-30 21:00:00| 47939.4352|\n",
      "|       44561|2011-09-30 21:00:00|  47348.815|\n",
      "|       44487|2011-09-30 21:00:00| 47155.5779|\n",
      "+------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_5 = spark.sql(\n",
    "    \"\"\" \n",
    "        SELECT SalesOrderID, OrderDate, TotalDue\n",
    "        FROM sales_order_header\n",
    "        WHERE MONTH(OrderDate) = 09 AND YEAR(OrderDate) = 2011\n",
    "            AND TotalDue > 1000\n",
    "        ORDER BY TotalDue DESC\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando resultados na pasta Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path = '/home/robson/repositorios/bike-manufacturing-company/bucket/gold'\n",
    "\n",
    "query_1.write.mode('overwrite').save(gold_path + '/query_1')\n",
    "query_2.write.mode('overwrite').save(gold_path + '/query_2')\n",
    "query_3.write.mode('overwrite').save(gold_path + '/query_3')\n",
    "query_4.write.mode('overwrite').save(gold_path + '/query_4')\n",
    "query_5.write.mode('overwrite').save(gold_path + '/query_5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
